# nginx config

# https://github.com/denji/nginx-tuning

# you must set worker processes based on your CPU cores, nginx does not benefit from setting more than that
# some last versions calculate it automatically
worker_processes auto;

# number of file descriptors used for nginx
# the limit for the maximum FDs on the server is usually set by the OS.
# if you don't set FD's then OS settings will be used which is by default 2000
worker_rlimit_nofile 100000;

# https://github.com/opentracing-contrib/nginx-opentracing/blob/master/doc/Reference.md
load_module modules/ngx_http_opentracing_module.so;

# provides the configuration file context in which the directives that affect connection processing are specified.
events {
    # determines how much clients will be served per worker
    # max clients = worker_connections * worker_processes
    # max clients is also limited by the number of socket connections available on the system (~64k)
    worker_connections 4000;

    # optimized to serve many clients with each thread, essential for linux -- for testing environment
    use epoll;

    # accept as many connections as possible, may flood worker connections if set too low -- for testing environment
    multi_accept on;
}

http {
  server_tokens off;

  opentracing on;
  opentracing_load_tracer /usr/local/lib/libjaegertracing_plugin.so /etc/jaeger-config.json;

  # cache informations about FDs, frequently accessed files
  # can boost performance, but you need to test those values
  open_file_cache max=200000 inactive=20s;
  open_file_cache_valid 30s;
  open_file_cache_min_uses 2;
  open_file_cache_errors on;

  # to boost I/O on HDD we can disable access logs
  access_log off;

  # let's also not log that if we don't have to
  error_log /dev/null;
  # need to debug something?
  # error_log /dev/stdout debug;

  # copies data between one FD and other from within the kernel
  # faster than read() + write()
  sendfile on;

  # send headers in one piece, it is better than sending them one by one
  tcp_nopush on;

  # don't buffer data sent, good for small data bursts in real time
  tcp_nodelay on;

  # reduce the data that needs to be sent over network -- for testing environment
  gzip on;
  gzip_static on;
  gzip_min_length 10240;
  gzip_comp_level 1;
  gzip_vary on;
  gzip_disable msie6;
  gzip_proxied expired no-cache no-store private auth;
  gzip_types
      # text/html is always compressed by HttpGzipModule
      text/css
      text/javascript
      text/xml
      text/plain
      text/x-component
      application/javascript
      application/x-javascript
      application/json
      application/xml
      application/rss+xml
      application/atom+xml
      font/truetype
      font/opentype
      application/vnd.ms-fontobject
      image/svg+xml;

  # allow the server to close connection on non responding client, this will free up memory
  reset_timedout_connection on;

  # request timed out -- default 60
  client_body_timeout 10;

  # if client stop responding, free up memory -- default 60
  send_timeout 2;

  # server will close connection after this time -- default 75
  keepalive_timeout 30;

  # number of requests client can make over keep-alive -- for testing environment
  keepalive_requests 100000;

  upstream frontend {
    zone upstream_frontend 64k;

    server frontend01:4000;
    server frontend02:4000;
    server frontend03:4000;

    server frontendbackup01:4000 backup;
    server frontendbackup02:4000 backup;

    least_conn;

    keepalive 32;
    keepalive_timeout 120s;
  }

  # very tight upstream timeouts
  proxy_connect_timeout 3s;
  proxy_send_timeout    3s;
  proxy_read_timeout    3s;

  server {
    listen 8080;
    server_name localhost;

    # we don't care about the intermediary location spans,
    # they don't add great value
    opentracing_trace_locations off;

    # serve favicon directly at the ingress, no need to pass that down to
    # our services if we have the file at hand;
    # does fallback to upstream in case of errors (file not found, permissions)
    location = /favicon.ico {
      expires 365d;
      root /static;
      try_files $uri @frontend;
    }

    # a pass-through location, so we can reuse @frontend for try_files
    location / {
      # https://serverfault.com/a/965779
      try_files /dev/null @frontend;
    }

    location @frontend {
      opentracing_propagate_context;
      opentracing_operation_name "$request_method $scheme://$host:$server_port$uri";
      opentracing_tag 'http.user_agent' $http_user_agent;
      opentracing_tag 'upstream.response_content_length' $upstream_response_length;
      opentracing_tag 'upstream.http.status_code' $upstream_status;

      # https://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive
      # but version 1.1 is also needed for tide anyway
      proxy_http_version 1.1;
      proxy_set_header Connection "";

      proxy_pass_request_headers on;
      proxy_pass http://frontend;
    }

    location = /stub_status {
      stub_status;
    }
  }
}
